
@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	urldate = {2019-05-14TZ},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366}
}

@article{zhang_neural_2000,
	title = {Neural networks for classification: a survey},
	volume = {30},
	issn = {1094-6977},
	shorttitle = {Neural networks for classification},
	doi = {10.1109/5326.897072},
	abstract = {Classification is one of the most active research and application areas of neural networks. The literature is vast and growing. This paper summarizes some of the most important developments in neural network classification research. Specifically, the issues of posterior probability estimation, the link between neural and conventional classifiers, learning and generalization tradeoff in classification, the feature variable selection, as well as the effect of misclassification costs are examined. Our purpose is to provide a synthesis of the published research in this area and stimulate further research interests and efforts in the identified topics.},
	number = {4},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Zhang, G. P.},
	month = nov,
	year = {2000},
	keywords = {Costs, Decision making, Humans, Input variables, Medical diagnosis, Medical diagnostic imaging, Network synthesis, Neural networks, Probability, Speech recognition, classification, conventional classifiers, feature variable selection, generalisation (artificial intelligence), generalization, learning, learning (artificial intelligence), misclassification costs, neural classifiers, neural nets, neural networks, pattern classification, posterior probability estimation},
	pages = {451--462}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2019-05-14TZ},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{amato_artificial_2013,
	title = {Artificial neural networks in medical diagnosis},
	volume = {11},
	issn = {1214-021X},
	url = {http://www.sciencedirect.com/science/article/pii/S1214021X14600570},
	doi = {10.2478/v10136-012-0031-x},
	abstract = {Summary
An extensive amount of information is currently available to clinical specialists, ranging from details of clinical symptoms to various types of biochemical data and outputs of imaging devices. Each type of data provides information that must be evaluated and assigned to a particular pathology during the diagnostic process. To streamline the diagnostic process in daily routine and avoid misdiagnosis, artificial intelligence methods (especially computer aided diagnosis and artificial neural networks) can be employed. These adaptive learning algorithms can handle diverse types of medical data and integrate them into categorized outputs. In this paper, we briefly review and discuss the philosophy, capabilities, and limitations of artificial neural networks in medical diagnosis through selected examples.},
	number = {2},
	urldate = {2019-05-14TZ},
	journal = {Journal of Applied Biomedicine},
	author = {Amato, Filippo and López, Alberto and Peña-Méndez, Eladia María and Vaňhara, Petr and Hampl, Aleš and Havel, Josef},
	month = jan,
	year = {2013},
	keywords = {artificial intelligence, artificial neural networks, cancer, cardiovascular diseases, diabetes, medical diagnosis},
	pages = {47--58}
}

@article{zhang_investigation_2001,
	title = {An investigation of neural networks for linear time-series forecasting},
	volume = {28},
	issn = {0305-0548},
	url = {http://www.sciencedirect.com/science/article/pii/S0305054800000332},
	doi = {10.1016/S0305-0548(00)00033-2},
	abstract = {This study examines the capability of neural networks for linear time-series forecasting. Using both simulated and real data, the effects of neural network factors such as the number of input nodes and the number of hidden nodes as well as the training sample size are investigated. Results show that neural networks are quite competent in modeling and forecasting linear time series in a variety of situations and simple neural network structures are often effective in modeling and forecasting linear time series.
Scope and purpose
Neural network capability for nonlinear modeling and forecasting has been established in the literature both theoretically and empirically. The purpose of this paper is to investigate the effectiveness of neural networks for linear time-series analysis and forecasting. Several research studies on neural network capability for linear problems in regression and classification have yielded mixed findings. This study aims to provide further evidence on the effectiveness of neural network with regard to linear time-series forecasting. The significance of the study is that it is often difficult in reality to determine whether the underlying data generating process is linear or nonlinear. If neural networks can compete with traditional forecasting models for linear data with noise, they can be used in even broader situations for forecasting researchers and practitioners.},
	number = {12},
	urldate = {2019-05-14TZ},
	journal = {Computers \& Operations Research},
	author = {Zhang, Guoqiang Peter},
	month = oct,
	year = {2001},
	keywords = {ARMA models, Neural networks, Time series forecasting},
	pages = {1183--1202}
}

@inproceedings{riedmiller_direct_1993,
	address = {San Francisco, CA, USA},
	title = {A direct adaptive method for faster backpropagation learning: the {RPROP} algorithm},
	isbn = {978-0-7803-0999-9},
	shorttitle = {A direct adaptive method for faster backpropagation learning},
	url = {http://ieeexplore.ieee.org/document/298623/},
	doi = {10.1109/ICNN.1993.298623},
	language = {en},
	urldate = {2019-04-29TZ},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Riedmiller, M. and Braun, H.},
	year = {1993},
	pages = {586--591}
}

@book{riedmiller_rprop_1994,
	title = {Rprop - {Description} and {Implementation} {Details}},
	abstract = {F31.64{\textgreater} 4 ij (t). This is based on a signdependent adaptation process, similar to the learning-rate adaptation in [4], [5]. 4  (t)  ij =  8 ? ? ! ? ? :  j  +   4  (t{\textbackslash}Gamma1)  ij  ; if  @E @w ij  (t{\textbackslash}Gamma1)    @E @w ij  (t)  ? 0  j  {\textbackslash}Gamma   4  (t{\textbackslash}Gamma1)  ij  ; if  @E @w ij  (t{\textbackslash}Gamma1)    @E @w ij  (t)  ! 0  4  (t{\textbackslash}Gamma1)  ij  ; else (2) where 0 ! j  {\textbackslash}Gamma  ! 1 ! j  + In words, the adaptation-rule works as follows: Every time the partial},
	author = {Riedmiller, Martin and Rprop, I.},
	year = {1994}
}

@techreport{riedmiller_rprop_1992,
	title = {{RPROP} - {A} {Fast} {Adaptive} {Learning} {Algorithm}},
	abstract = {In this paper, a new learning algorithm, RPROP, is proposed. To overcome the inherent disadvantages of the pure gradient-descent technique of the original backpropagation procedure, RPROP performs an adaptation of the weight update-values according to the behaviour of the errorfunction. The results of RPROP on several learning tasks are shown in comparison to other well-known adaptive learning algorithms. 1 Introduction  Backpropagation is the most widely used algorithm for supervised learning with multilayered feed-forward networks. The basic idea of the backpropagation learning algorithm is the repeated application of the chain rule to compute the influence of each weight in the network with respect to an arbitrary errorfunction E [1]: @E @w ij  =  @E @a i  @a i  @net i  @net i  @w ij  (1) where w ij is the weight from neuron j to neuron i, a i is the activation value and net i  is the weighted sum of the inputs of neuron i. Once the partial derivative for each weight is known, the a...},
	institution = {Proc. of ISCIS VII), Universitat},
	author = {Riedmiller, Martin and Braun, Heinrich},
	year = {1992}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {PROBEN}1 - {A} {Set} of {Neural} {Network} {Benchmark} {Problems} and {Benchmarking} {Rules}},
	url = {https://www.researchgate.net/publication/2738156_PROBEN1_-_A_Set_of_Neural_Network_Benchmark_Problems_and_Benchmarking_Rules},
	urldate = {2019-04-24TZ}
}

@article{balazs_cascade-correlation_2009,
	title = {Cascade-{Correlation} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Cascade-{Correlation} {Neural} {Networks}},
	abstract = {This paper is an overview of cascade-correlation neural networks which form a specific class inside neural network function approximators. They are based on a special architecture which autonomously adapts to the application and makes the training much more efficient than the widely used backpropagation algorithm. This survey describes the cascade-correlation architecture variants, shows important applications and points to many future research directions.},
	author = {Balázs, Gábor},
	month = dec,
	year = {2009}
}

@article{schmidhuber_deep_2015,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep {Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2019-04-15TZ},
	journal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	month = jan,
	year = {2015},
	note = {arXiv: 1404.7828},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {85--117}
}

@article{phatak_connectivity_1994,
	title = {Connectivity and performance tradeoffs in the cascade correlation learning architecture},
	volume = {5},
	issn = {1045-9227},
	doi = {10.1109/72.329690},
	abstract = {The cascade correlation is a very flexible, efficient and fast algorithm for supervised learning. It incrementally builds the network by adding hidden units one at a time, until the desired input/output mapping is achieved. It connects all the previously installed units to the new unit being added. Consequently, each new unit in effect adds a new layer and the fan-in of the hidden and output units keeps on increasing as more units get added. The resulting structure could be hard to implement in VLSI, because the connections are irregular and the fan-in is unbounded. Moreover, the depth or the propagation delay through the resulting network is directly proportional to the number of units and can be excessive. We have modified the algorithm to generate networks with restricted fan-in and small depth (propagation delay) by controlling the connectivity. Our results reveal that there is a tradeoff between connectivity and other performance attributes like depth, total number of independent parameters, and learning time.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Phatak, D. S. and Koren, I.},
	month = nov,
	year = {1994},
	keywords = {Intelligent networks, Machine learning, Machine learning algorithms, Neural networks, Propagation delay, Supervised learning, Topology, VLSI implementation, Very large scale integration, cascade correlation, connectivity, feedforward neural nets, hidden units, input/output mapping, learning (artificial intelligence), learning time, network topology, parallel architectures, performance tradeoffs, propagation delay, supervised learning, topology},
	pages = {930--935}
}

@inproceedings{pannu_:_2015,
	title = {: 2277-3754 {ISO} 9001 : 2008},
	abstract = {In the future, intelligent machines will replace or enhance human capabilities in many areas. Artificial intelligence is the intelligence exhibited by machines or software. It is the subfield of computer science. Artificial Intelligence is becoming a popular field in computer science as it has enhanced the human life in many areas. Artificial intelligence in the last two decades has greatly improved performance of the manufacturing and service systems. Study in the area of artificial intelligence has given rise to the rapidly growing technology known as expert system. Application areas of Artificial Intelligence is having a huge impact on various fields of life as expert system is widely used these days to solve the complex problems in various areas as science, engineering, business, medicine, weather forecasting. The areas employing the technology of Artificial Intelligence have seen an increase in the quality and efficiency. This paper gives an overview of this technology and the application areas of this technology. This paper will also explore the current use of Artificial Intelligence technologies in the PSS design to damp the power system oscillations caused by interruptions, in Network Intrusion for protecting computer and communication networks from intruders, in the medical areamedicine, to improve hospital inpatient care, for medical image classification, in the accounting databases to mitigate the problems of it and in the computer games.},
	author = {Pannu, Avneet},
	year = {2015},
	keywords = {Artificial Intelligence, CNS disorder, Computer vision, Dental Intrusion, Expert system, Intrusion detection system, Medical Image, PC game, Physical symbol system, Projections and Predictions, Published Database, Ruellia sp. 22 Daniel 10632, Telecommunications network, WebDAV, computer science}
}

@incollection{littmann_generalization_1993,
	title = {Generalization {Abilities} of {Cascade} {Network} {Architecture}},
	url = {http://papers.nips.cc/paper/665-generalization-abilities-of-cascade-network-architecture.pdf},
	urldate = {2019-03-11TZ},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 5},
	publisher = {Morgan-Kaufmann},
	author = {Littmann, E. and Ritter, H.},
	editor = {Hanson, S. J. and Cowan, J. D. and Giles, C. L.},
	year = {1993},
	pages = {188--195}
}

@inproceedings{song_improvement_2011,
	title = {The {Improvement} of {Neural} {Network} {Cascade}-correlation {Algorithm} and its {Application} in {Picking} {Seismic} {First} {Break}},
	url = {http://www.earthdoc.org/publication/publicationdetails/?publication=50680},
	doi = {10.3997/2214-4609.20149418},
	abstract = {We developed a first break auto-picking method based on cascade-correlation neural network. Different from the original cascade-correlation algorithm, the improved algorithm begins with an appropriate BP network architecture (exits hidden units). In addition, in order to avoid weight-illgrowth, and reduce the complexity of network, a regularization term is added to the correlation when training candidate hidden units. Empirical study shows that improved cascade-correlation algorithm has faster convergence speed and stronger generalization ability. Five attributes, including power ratio, maximum amplitude of the peak, frequency, curve length ratio, and adjacent seismic channel correlation are discussed. Cross plots show that these five attributes are adequate to statistically separate the first breaks and the non first breaks. The neural network first break picking method of this paper has achieved good effect in testing of actual seismic data.},
	language = {English},
	urldate = {2019-03-11TZ},
	author = {Song, J. G. and Zeng, W. H. and Xu, Y. and Xu, W. X.},
	month = may,
	year = {2011}
}

@misc{noauthor_cascade_nodate,
	title = {Cascade network architectures - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/226955},
	urldate = {2019-03-11TZ}
}

@misc{noauthor_evaluation_nodate,
	title = {Evaluation of constructive neural networks with cascaded architectures - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006300},
	urldate = {2019-03-11TZ}
}

@inproceedings{kwok_experimental_1993,
	title = {Experimental analysis of input weight freezing in constructive neural networks},
	doi = {10.1109/ICNN.1993.298610},
	abstract = {An important research problem in constructive network algorithms is how to train the new network after the addition of a hidden unit. Some previous empirical analyses performed on the cascade-correlation architecture indicate that the effectiveness of freezing is different for different problem domains and hence is not conclusive. A series of experiments with the single-hidden-layer network on a number of artificial pattern classification problems is described. The performance of the network is compared with and without input weight freezing, and against standard backpropagation. Drawbacks with freezing are identified, and some directions for future work are discussed.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	author = {Kwok, T.- and Yeung, D.-},
	month = mar,
	year = {1993},
	keywords = {Artificial neural networks, Ash, Computational efficiency, Computer architecture, Computer science, Intelligent networks, Neural networks, Pattern classification, Performance analysis, Testing, backpropagation, cascade-correlation architecture, constructive neural networks, empirical analyses, hidden unit, input weight freezing, neural nets, pattern classification problems, pattern recognition, problem domains, single-hidden-layer network},
	pages = {511--516 vol.1}
}

@inproceedings{treadgold_extending_1997,
	title = {Extending {CasPer}: {A} {Regression} {Survey}},
	shorttitle = {Extending {CasPer}},
	abstract = {The CasPer algorithm is a constructive neural network algorithm. CasPer creates cascade network architectures in a similar manner to Cascade Correlation. CasPer, however, uses a modified form of the RPROP algorithm, termed Progressive RPROP, to train the whole network after the addition of each new hidden neuron. Previous work with CasPer has shown that it builds networks which generalise better than CasCor, often using less hidden neurons. This work adds two extensions to CasPer. First, an enhancement to the RPROP algorithm, SARPROP, is used to train newly installed hidden neurons. The second extension involves the use of a pool of hidden neurons, each trained using SARPROP, with the best performing neuron selected for insertion into the network. These extensions are benchmarked on a number of regression problems and are shown to result in CasPer producing networks which generalise better than those produced by the original CasPer algorithm.  1 Introduction  The CasPer algorithm has b...},
	booktitle = {Int. {Conf}. {On} {Neural} {Information} {Processing}},
	author = {Treadgold, N. K. and Gedeon, T. D.},
	year = {1997}
}

@inproceedings{thivierge_dual-phase_2003,
	title = {A dual-phase technique for pruning constructive networks},
	volume = {1},
	doi = {10.1109/IJCNN.2003.1223407},
	abstract = {An algorithm for performing simultaneous growing and pruning of cascade-correlation (CC) neural networks is introduced and tested. The algorithm adds hidden units as in standard CC, and removes unimportant connections by using optimal brain damage (OBD) in both the input and output phases of CC. To this purpose, OBD was adapted to prune weights according to two separate objective functions that are used in CC to train the network, respectively. Application of the new algorithm to two databases of the PROBEN1 benchmarks reveals that this new dual-phase pruning technique is effective in significantly reducing the size of CC networks, while providing a speed-up in learning times and improvements in generalization over novel test sets.},
	booktitle = {Proceedings of the {International} {Joint} {Conference} on {Neural} {Networks}, 2003.},
	author = {Thivierge, J. P. and Rivest, F. and Shultz, T. R.},
	month = jul,
	year = {2003},
	keywords = {Benchmark testing, Biological neural networks, Computer network management, Computer science, Databases, Network topology, PROBEN1 benchmarks, Performance evaluation, Potential well, Psychology, Quality management, cascade-correlation neural networks, constructive networks pruning, databases, dual-phase pruning technique, generalisation (artificial intelligence), input phases, learning (artificial intelligence), neural nets, optimal brain damage, output phases},
	pages = {559--564 vol.1}
}

@misc{noauthor_pdf_nodate-1,
	title = {({PDF}) {Cascade}-{Correlation} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {({PDF}) {Cascade}-{Correlation} {Neural} {Networks}},
	url = {https://www.researchgate.net/publication/228572120_Cascade-Correlation_Neural_Networks_A_Survey},
	abstract = {PDF {\textbar} This paper is an overview of cascade-correlation neural networks which form a specific class inside neural network function approximators. They are based on a special architecture which autonomously adapts to the application and makes the training much more efficient than the...},
	language = {en},
	urldate = {2019-03-11TZ},
	journal = {ResearchGate}
}

@article{tetko_enhancement_1997,
	title = {An {Enhancement} of {Generalization} {Ability} in {Cascade} {Correlation} {Algorithm} by {Avoidance} of {Overfitting}/{Overtraining} {Problem}},
	volume = {6},
	issn = {1573-773X},
	url = {https://doi.org/10.1023/A:1009610808553},
	doi = {10.1023/A:1009610808553},
	abstract = {The current study investigates a method for avoidance of an overfitting/overtraining problem in Artificial Neural Network (ANN) based on a combination of two algorithms: Early Stopping and Ensemble averaging (ESE). We show that ESE provides an improvement of the prediction ability of ANN trained according to Cascade Correlation Algorithm. A simple algorithm to estimate the generalization ability of the method according to the Leave-One-Out technique is proposed and discussed. In the accompanying paper the problem of optimal selection of training cases is considered for accelerated learning of the ESE method.},
	language = {en},
	number = {1},
	urldate = {2019-03-11TZ},
	journal = {Neural Processing Letters},
	author = {Tetko, Igor V. and Villa, Alessandro E.P.},
	month = aug,
	year = {1997},
	keywords = {cascade correlation algorithm, early stopping, overfitting, overtraining},
	pages = {43--50}
}

@article{kaur_modified_nodate,
	title = {Modified {Cascade}-2 {Algorithm} with {Adaptive} {Slope} {Sigmoidal} {Function}},
	abstract = {Cascade-2 algorithm is a variant of well-known cascade-correlation algorithm that is widely investigated constructive training algorithm for designing cascade feedforward neural networks. This paper proposes a modified Cascade-2 algorithm with adaptive slope sigmoidal function (MC2AASF). The algorithm emphasizes on architectural adaptation and functional adaptation during learning. This algorithm is a constructive approach of designing cascade architecture. To achieve functional adaptation, the slope of the sigmoidal function is adapted during training. One simple variant is derived from MC2AASF is where the slope parameter of sigmoidal function used at the hidden layers’ nodes is fixed to unity. Both the variants are compared to each other on five function approximation tasks. Simulation results show that adaptive slope sigmoidal function presents several advantages over standard fixed shape sigmoidal function, resulting in increasing flexibility, smoother learning, better generalization performance and better convergence.},
	language = {en},
	author = {Kaur, Jaswinder and Sharma, Sudhir Kumar},
	pages = {5}
}

@inproceedings{riley_improving_2010,
	address = {London, UK},
	title = {Improving the performance of cascade correlation neural networks on multimodal functions},
	isbn = {978-988-18210-8-9},
	url = {http://www.iaeng.org/publication/WCE2010/WCE2010_pp1980-1986.pdf},
	abstract = {Intrinsic qualities of the cascade correlation algorithm make it a popular choice for many researchers wishing to utilize neural networks. Problems arise when the outputs required are highly multimodal over the input domain. The mean squared error of the approximation increases significantly as the number of modes increases. By applying ensembling and early stopping, we show that this error can be reduced by a factor of three. We also present a new technique based on subdivision that we call patchworking. When used in combination with early stopping and ensembling the mean 
improvement in error is over 10 in some cases.},
	language = {en},
	urldate = {2019-03-11TZ},
	publisher = {International Association of Engineers IAENG},
	author = {Riley, Mike and Jenkins, Karl W. and Thompson, Chris P.},
	month = jul,
	year = {2010},
	pages = {1980--1986}
}

@article{masters_revisiting_2018,
	title = {Revisiting {Small} {Batch} {Training} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.07612},
	abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
	urldate = {2019-03-11TZ},
	journal = {arXiv:1804.07612 [cs, stat]},
	author = {Masters, Dominic and Luschi, Carlo},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.07612},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{fahlman_empirical_nodate,
	title = {An {Empirical} {Study} of {Learning} {Speed} in {Back}-{Propagation} {Networks}},
	abstract = {Most connectionist or "neural network" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind.},
	language = {en},
	author = {Fahlman, Scott E},
	pages = {19}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2019-03-11TZ},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning}
}

@misc{noauthor_citeseerx_nodate,
	title = {{CiteSeerX} — {RPROP} - {A} {Fast} {Adaptive} {Learning} {Algorithm}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.4576},
	urldate = {2019-03-11TZ}
}

@article{prechelt_investigation_1997,
	title = {Investigation of the {CasCor} {Family} of {Learning} {Algorithms}},
	volume = {10},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608096001153},
	doi = {10.1016/S0893-6080(96)00115-3},
	abstract = {Sixlearning algorithms are investigated and compared empirically. All of them are basedon variantsof the candidatetrainingideaof the CascadeCorrelationmethod.Thecomparisonwasperjomsedusing42 differentdatasets from thePROBENIbenchmarkcollection.Theresultsindicate:(I)for theseproblemsit is slightlybetternot to cascade the hidden units; (2) error minimizationcandiakztetraining is better than covariancemaximizationfor regression problemxbut may be a little worsefor classificationproblems;(3)for most learningtasks,consideringvaliaiztionset errorsduringthe selectionof the best candidatewi{\textasciitilde}lnot lead to improvednetworks,butfor afew tasksit will O 1997 Elsevier ScienceLtd.},
	language = {en},
	number = {5},
	urldate = {2019-03-11TZ},
	journal = {Neural Networks},
	author = {Prechelt, Lutz},
	month = jul,
	year = {1997},
	pages = {885--896}
}

@article{prechelt_investigation_1997-1,
	title = {Investigation of the {CasCor} {Family} of {Learning} {Algorithms}},
	volume = {10},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608096001153},
	doi = {10.1016/S0893-6080(96)00115-3},
	abstract = {Six learning algorithms are investigated and compared empirically. All of them are based on variants of the candidate training idea of the Cascade Correlation method. The comparison was performed using 42 different datasets from the PROBEN1 benchmark collection. The results indicate: (1) for these problems it is slightly better not to cascade the hidden units; (2) error minimization candidate training is better than covariance maximization for regression problems but may be a little worse for classification problems; (3) for most learning tasks, considering validation set errors during the selection of the best candidate will not lead to improved networks, but for a few tasks it will. © 1997 Elsevier Science Ltd.},
	number = {5},
	urldate = {2019-03-11TZ},
	journal = {Neural Networks},
	author = {Prechelt, Lutz},
	month = jul,
	year = {1997},
	keywords = {Additive learning, Cascade correlation, Constructive learning, Cross validation, Empirical study},
	pages = {885--896}
}

@article{sharma_constructive_2010,
	title = {{CONSTRUCTIVE} {NEURAL} {NETWORKS}: {A} {REVIEW}},
	volume = {2},
	abstract = {In conventional neural networks, we have to define the architecture prior to training but in constructive neural networks the network architecture is constructed during the training process. In this paper, we review constructive neural network algorithms that constructing feedforward architecture for regression problems. Cascade-Correlation algorithm (CCA) is a well-known and widely used constructive algorithm. Cascade 2 algorithm is a variant of CCA that is found to be more suitable for regression problems and is reviewed in this paper. We review our recently proposed two constructive algorithms that emphasize on architectural adaptation and functional adaptation during training. To achieve functional adaptation, the slope of the sigmoidal function is adapted during learning. The algorithm determines not only the optimum number of hidden layer nodes, as also the optimum value of the slope parameter of sigmoidal function. The role of adaptive sigmoidal activation function has been verified in constructive neural networks for better generalization performance and lesser training time.},
	language = {en},
	journal = {International Journal of Engineering Science and Technology},
	author = {Sharma, Sudhir Kumar and Chandra, Pravin},
	year = {2010},
	pages = {9}
}

@article{nissen_large_nodate,
	title = {Large {Scale} {Reinforcement} {Learning} using {Q}-{SARSA}(λ) and {Cascading} {Neural} {Networks}},
	language = {en},
	author = {Nissen, Steﬀen},
	pages = {264}
}

@article{ahmed_artificial_2005,
	title = {Artificial neural networks for diagnosis and survival prediction in colon cancer},
	volume = {4},
	issn = {1476-4598},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1208946/},
	doi = {10.1186/1476-4598-4-29},
	abstract = {ANNs are nonlinear regression computational devices that have been used for over 45 years in classification and survival prediction in several biomedical systems, including colon cancer. Described in this article is the theory behind the three-layer free forward artificial neural networks with backpropagation error, which is widely used in biomedical fields, and a methodological approach to its application for cancer research, as exemplified by colon cancer. Review of the literature shows that applications of these networks have improved the accuracy of colon cancer classification and survival prediction when compared to other statistical or clinicopathological methods. Accuracy, however, must be exercised when designing, using and publishing biomedical results employing machine-learning devices such as ANNs in worldwide literature in order to enhance confidence in the quality and reliability of reported data.},
	urldate = {2019-02-25TZ},
	journal = {Molecular Cancer},
	author = {Ahmed, Farid E},
	month = aug,
	year = {2005},
	pmid = {16083507},
	pmcid = {PMC1208946},
	pages = {29}
}

@inproceedings{alhady_butterfly_2018,
	series = {Lecture {Notes} in {Mechanical} {Engineering}},
	title = {Butterfly {Species} {Recognition} {Using} {Artificial} {Neural} {Network}},
	isbn = {978-981-10-8788-2},
	abstract = {In 2017, there are about 20,000 species of butterfly has been discovered all over the world. Butterfly is well known because of its beautiful wings pattern and its benefits to the environment. In this research, butterfly species recognition is automated using artificial intelligence. Pattern on the butterfly wings is used as a parameter to determine the species of the butterfly. The butterfly image is captured and the background of the image is removed to make the recognition process easier. Local binary pattern (LBP) descriptor is then applied to the processed image and a histogram consist of image information is computed. Artificial Neural Network (ANN) is used to classify the image. Two types of butterfly species were selected namely ideopsis vulgaris and hypolimnas bolina. Both of the species have been correctly identify with accuracy of 90\% (for ideopsis vulgaris) and 100\% (for hypolimnas bolina).},
	language = {en},
	booktitle = {Intelligent {Manufacturing} \& {Mechatronics}},
	publisher = {Springer Singapore},
	author = {Alhady, S. S. N. and Kai, Xin Yong},
	editor = {Hassan, Mohd Hasnun Arif},
	year = {2018},
	keywords = {Artificial intelligence, Neural network, Pattern recognition},
	pages = {449--457}
}

@misc{noauthor_tesla_2019,
	title = {Tesla {Autopilot}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Tesla_Autopilot&oldid=883954355},
	abstract = {Tesla Autopilot also known as  Enhanced Autopilot after a second hardware version started to be shipped, is an advanced driver-assistance system feature offered by Tesla that has lane centering, adaptive cruise control, self-parking, ability to automatically change lanes with driver confirmation, and enables the car to be summoned to and from a garage or parking spot. 
As an upgrade above and beyond Enhanced Autopilot's capabilities, the company's stated intent is to offer full self-driving at a future time, acknowledging that legal, regulatory, and technical hurdles must be overcome to achieve this goal.},
	language = {en},
	urldate = {2019-02-25TZ},
	journal = {Wikipedia},
	month = feb,
	year = {2019},
	note = {Page Version ID: 883954355}
}

@misc{noauthor_autopilot_nodate,
	title = {Autopilot},
	url = {https://www.tesla.com/autopilot},
	abstract = {All Tesla vehicles produced in our factory, including Model 3, have the hardware needed for full self-driving capability at a safety level substantially greater than that of a human driver.},
	urldate = {2019-02-25TZ}
}

@misc{noauthor_autopilot_nodate-1,
	title = {Autopilot},
	url = {https://www.tesla.com/autopilot},
	abstract = {Alla Tesla-bilar som tillverkas i vår fabrik, inklusive Model 3, har all hårdvara som behövs för total självkörningsförmåga med en säkerhetsnivå som är betydligt mycket högre än den för en mänsklig förare.},
	urldate = {2019-02-25TZ}
}

@article{lahnajarvi_evaluation_2002,
	title = {Evaluation of constructive neural networks with cascaded architectures},
	volume = {48},
	issn = {09252312},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231201006300},
	doi = {10.1016/S0925-2312(01)00630-0},
	abstract = {In this study, we have investigated ÿve di erent constructive neural network algorithms, of which four were methods found in the literature and one was our own recently developed algorithm. The algorithms that were studied were Cascade-Correlation, Modiÿed Cascade-Correlation, Cascade, Cascade Network, and our own recently developed Fixed Cascade Error. The investigated algorithms have many similarities: they all have a cascaded architecture and they automatically increase the size of the neural network by adding new hidden units to the network as the training proceeds. Furthermore, the networks are trained in a layer-by-layer style, i.e. as the hidden units are installed in the network, their input weights are frozen so that they do not change in the later stages of the network training. The basic versions of the algorithms (which use only one randomly initialized candidate unit in the hidden unit training) were improved during the course of this research by adding a deterministic initialization method and the utilization of multiple candidate units in the training phase of the hidden units. The key idea of the deterministic initialization method is to create a large pool of randomly initialized hidden units, of which only the best unit is further trained and installed in the network. On the other hand, when we utilize multiple candidate units, we train a number of candidate units to the ÿnal solution, after which the best one of them is selected to be installed as a hidden unit in the active network. The numerical simulations show that especially the multiple candidate unit versions of the algorithms produce usually better results than the basic versions of the algorithms. In addition, the computational costs of the algorithms do not increase when using the deterministic initialization method, but in most cases we can even reduce the computational costs needed for the network training. Moreover, it should be noticed that our own algorithm produces rather often the best performance level among the investigated algorithms. c 2002 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-4},
	urldate = {2019-02-01TZ},
	journal = {Neurocomputing},
	author = {Lahnajärvi, Jani J.T. and Lehtokangas, Mikko I. and Saarinen, Jukka P.P.},
	month = oct,
	year = {2002},
	pages = {573--607}
}

@techreport{baluja_reducing_1994,
	address = {Fort Belvoir, VA},
	title = {Reducing {Network} {Depth} in the {Cascade}-{Correlation} {Learning} {Architecture},:},
	shorttitle = {Reducing {Network} {Depth} in the {Cascade}-{Correlation} {Learning} {Architecture},},
	url = {http://www.dtic.mil/docs/citations/ADA289352},
	abstract = {The Cascade-Correlation learning algorithm constructs a multi-layer artificial neural network as it learns to perform a given task. The resulting network's size and topology are chosen specifically for this task. In the resulting "cascade" networks, each new hidden unit receives incoming connections from all input and pre-existing hidden units. In effect, each new unit adds a new layer to the network. This allows CascadeCorrelation to create complex feature detectors, but it typically results in a network that is deeper, in terms of the longest path from input to output, than is necessary to solve the problem efficiently. In this paper we investigate a simple variation of Cascade-Correlation that will build deep nets if necessary, but that is biased toward minimizing network depth. We demonstrate empirically, across a range of problems, that this simple technique can reduce network depth, often dramatically. However, we show that this technique does not, in general, reduce the total number of weights or improve the generalization ability of the resulting networks.},
	language = {en},
	urldate = {2019-01-28TZ},
	institution = {Defense Technical Information Center},
	author = {Baluja, Shumeet and Fahlman, Scott E.},
	month = oct,
	year = {1994},
	doi = {10.21236/ADA289352}
}
@incollection{fahlman_cascade-correlation_1990,
	title = {The {Cascade}-{Correlation} {Learning} {Architecture}},
	url = {http://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture.pdf},
	urldate = {2019-01-28TZ},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 2},
	publisher = {Morgan-Kaufmann},
	author = {Fahlman, Scott E. and Lebiere, Christian},
	editor = {Touretzky, D. S.},
	year = {1990},
	pages = {524--532}
}

@article{fahlman_empirical_nodate,
	title = {An {Empirical} {Study} of {Learning} {Speed} in {Back}-{Propagation} {Networks}},
	abstract = {Most connectionist or "neural network" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind.},
	language = {en},
	author = {Fahlman, Scott E},
	pages = {19}
}